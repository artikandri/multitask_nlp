{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8095eb4b",
   "metadata": {},
   "source": [
    "# NKJP Podkorpus milionowy\n",
    "\n",
    "Wykorzystane dane oryginalne z http://clip.ipipan.waw.pl/NationalCorpusOfPolish?action=AttachFile&do=get&target=NKJP-PodkorpusMilionowy-1.2.tar.gz.\n",
    "\n",
    "Stworzenie pliku `.iob`, który będzie zawierał dane ze wszystkich dokumentów wraz z anotacją NER oraz POS (już po dezambiguacji).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844b132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "sys.path.append('../../../')\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from multitask_nlp.settings import DATASETS_DIR\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "data_path  = DATASETS_DIR / 'nkjp1m' \n",
    "dataset_path = data_path / 'NKJP-PodkorpusMilionowy-1.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "372706c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dirs = [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87351746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67b9e137a874c7eb0f47ac564515a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3889 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Token:\n",
    "    orth: str\n",
    "    base: str\n",
    "    pos_tag: str = None\n",
    "    ner_tags: List[str] = None\n",
    "    ner_tag: str = None\n",
    "        \n",
    "    def ner_from_tag_list(self, tag_list):\n",
    "        self.ner_tags = tag_list\n",
    "        self.ner_tag = self.get_full_ner_tag()        \n",
    "        \n",
    "    def get_full_ner_tag(self):        \n",
    "        processed_tags = []\n",
    "        for tag in self.ner_tags:\n",
    "            split_tag = tag.split('-')\n",
    "            if len(split_tag) > 1 and split_tag[0] in ['B', 'I']:\n",
    "                processed_tags.append('-'.join(split_tag[1:]))\n",
    "            else:\n",
    "                processed_tags.append(tag)\n",
    "\n",
    "        sorted_tags = sorted(zip(self.ner_tags, processed_tags), key=lambda item: item[1])\n",
    "        sorted_tags = [i[0] for i in sorted_tags]\n",
    "        full_ner_tag = '#'.join(sorted_tags)\n",
    "        \n",
    "#         print(processed_tags)\n",
    "#         print(sorted_tags)\n",
    "#         print(full_ner_tag)\n",
    "#         print()\n",
    "        return full_ner_tag\n",
    "\n",
    "@dataclass\n",
    "class NER_Tag:\n",
    "    id: str\n",
    "    tag: str\n",
    "    target: str\n",
    "\n",
    "sentence_tag = '{http://www.tei-c.org/ns/1.0}s'\n",
    "segment_tag = '{http://www.tei-c.org/ns/1.0}seg'\n",
    "id_tag = '{http://www.w3.org/XML/1998/namespace}id'\n",
    "ptr_tag = '{http://www.tei-c.org/ns/1.0}ptr'\n",
    "\n",
    "documents_data = []\n",
    "\n",
    "for document_dir in tqdm(document_dirs):\n",
    "    morpho_filepath = dataset_path/ document_dir / 'ann_morphosyntax.xml'\n",
    "    named_filepath = dataset_path/ document_dir / 'ann_named.xml'\n",
    "    \n",
    "    tree = ET.parse(morpho_filepath)\n",
    "    text_node = tree.getroot()[1][1]\n",
    "\n",
    "    sentences = OrderedDict()\n",
    "    \n",
    "    for sentence_chunk in text_node.iter(sentence_tag):\n",
    "        sentence_id = sentence_chunk.attrib[id_tag]\n",
    "        tokens_in_sentence = OrderedDict()       \n",
    "        \n",
    "        for seg in sentence_chunk.iter(segment_tag):\n",
    "            seg_id = seg.attrib[id_tag]\n",
    "            fs_morph_node = seg[0]\n",
    "            f_nodes = fs_morph_node.findall('{http://www.tei-c.org/ns/1.0}f')\n",
    "            \n",
    "            orth = None\n",
    "            base = None\n",
    "            pos_tag = None\n",
    "            for f_node in f_nodes:\n",
    "                if f_node.attrib['name'] == 'orth':\n",
    "                    orth = f_node[0].text\n",
    "                elif f_node.attrib['name'] == 'disamb':\n",
    "                    disamb_node = f_node\n",
    "                    disamb_text = disamb_node[0][1][0].text\n",
    "\n",
    "                    disamb_text_split = disamb_text.split(':')\n",
    "                    base = disamb_text_split[0]\n",
    "                    pos_tag = ':'.join(disamb_text_split[1:])\n",
    "            \n",
    "            \n",
    "            token = Token(orth=orth, base=base, pos_tag=pos_tag)\n",
    "            tokens_in_sentence[seg_id] = token\n",
    "        \n",
    "        sentences[sentence_id] = tokens_in_sentence\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(named_filepath)\n",
    "        text_node = tree.getroot()[1][1]\n",
    "\n",
    "        for sentence_chunk in text_node.iter(sentence_tag):\n",
    "            sentence_id = sentence_chunk.attrib[id_tag]\n",
    "            morph_sentence_id = sentence_id.replace('named', 'morph')\n",
    "\n",
    "            ner_tags = []\n",
    "            for seg in sentence_chunk.iter(segment_tag):\n",
    "                seg_id = seg.attrib[id_tag]           \n",
    "                fs_morph_node = seg[0]\n",
    "                f_nodes = fs_morph_node.findall('{http://www.tei-c.org/ns/1.0}f')\n",
    "\n",
    "                ner_tag_type = None\n",
    "                ner_tag_subtype = None\n",
    "                for f_node in f_nodes:\n",
    "                    if f_node.attrib['name'] == 'type':\n",
    "                        ner_tag_type = f_node[0].attrib['value']\n",
    "                    if f_node.attrib['name'] == 'subtype':\n",
    "                        ner_tag_subtype = f_node[0].attrib['value']\n",
    "\n",
    "                ner_tag = f'{ner_tag_type}'\n",
    "                if ner_tag_subtype is not None:\n",
    "                    ner_tag += f'-{ner_tag_subtype}'\n",
    "\n",
    "                ptr_targets = [ptr.attrib['target']  for ptr in seg.findall(ptr_tag)]            \n",
    "                if ner_tag is not None:\n",
    "                    for ptr_target in ptr_targets:\n",
    "                        ner_tag_obj = NER_Tag(id=seg_id, tag=ner_tag, target=ptr_target)\n",
    "                        ner_tags.append(ner_tag_obj)\n",
    "\n",
    "            basic_ner_tags = [nt for nt in ner_tags if len(nt.target.split('#')) > 1]\n",
    "            sub_ner_tags = [nt for nt in ner_tags if len(nt.target.split('#')) == 1]\n",
    "\n",
    "            for nt in basic_ner_tags:\n",
    "                morph_seg_id = nt.target.split('#')[1]\n",
    "\n",
    "                ner_tag_list = []\n",
    "                for sub_nt in sub_ner_tags:\n",
    "                    if sub_nt.target == nt.id:\n",
    "                        ner_tag_list.append(sub_nt.tag)\n",
    "\n",
    "                ner_tag_list.append(nt.tag)\n",
    "\n",
    "                sentences[morph_sentence_id][morph_seg_id].ner_from_tag_list(ner_tag_list)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "            \n",
    "    for s_id, s_tokens in sentences.items():\n",
    "        prev_token = None\n",
    "        for t_id, curr_token in s_tokens.items():\n",
    "            if curr_token.ner_tags == None:\n",
    "                curr_token.ner_tag = 'O'\n",
    "            else:\n",
    "                if prev_token is not None and prev_token.ner_tags is not None:\n",
    "                    prev_original_tags = ['-'.join(t.split('-')[1:]) for t in prev_token.ner_tags]\n",
    "                    \n",
    "                    iob_ner_tags = []\n",
    "                    for t in curr_token.ner_tags:\n",
    "                        if t in prev_original_tags:\n",
    "                            iob_ner_tags.append(f'I-{t}')\n",
    "                        else:\n",
    "                            iob_ner_tags.append(f'B-{t}')\n",
    "                    \n",
    "                    curr_token.ner_from_tag_list(iob_ner_tags)\n",
    "                    \n",
    "                else:\n",
    "                    curr_token.ner_from_tag_list([f'B-{tag}' for tag in curr_token.ner_tags])     \n",
    "                    \n",
    "            prev_token = curr_token\n",
    "    \n",
    "    documents_data.append((document_dir, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5780b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path / 'nkjp1m.iob', 'w', encoding='utf-8') as f:\n",
    "    f.write('-DOCSTART CONFIG FEATURES orth base ctag\\n')\n",
    "    for document_dir, sentences in documents_data:\n",
    "        f.write(f'-DOCSTART FILE ./{document_dir}\\n')\n",
    "        for sent_id, tokens in sentences.items():\n",
    "            for token_id, token in tokens.items():\n",
    "                f.write(f'{token.orth}\\t{token.base}\\t{token.pos_tag}\\t{token.ner_tag}\\n')\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
